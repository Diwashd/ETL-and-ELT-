{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "450538bb",
   "metadata": {},
   "source": [
    "### Simplified Notes on Extracting Data from Structured Sources\n",
    "\n",
    "#### 1. **Introduction to Data Extraction**\n",
    "- The first step in any data pipeline is extracting data from a **source system**.\n",
    "- This can include files, databases, APIs, or even web scraping.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Types of Source Systems**\n",
    "- **Static File Types**: CSV, Parquet, JSON\n",
    "- **Dynamic Sources**: SQL databases\n",
    "- **External Sources**: APIs (for third-party data), Web scraping\n",
    "- **Enterprise Systems**: Data lakes and data warehouses\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Reading Parquet Files**\n",
    "- **Parquet** is a fast, columnar storage file format — more efficient than CSV.\n",
    "- Use `pd.read_parquet(\"file_path\", engine=\"fastparquet\")` to read a Parquet file into a DataFrame.\n",
    "- Similar to reading CSV files, but faster for large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Extracting from SQL Databases**\n",
    "- Use `pandas.read_sql(query, connection)` to load data from SQL into a DataFrame.\n",
    "- First, create a **connection object** using SQLAlchemy:\n",
    "  ```python\n",
    "  from sqlalchemy import create_engine\n",
    "  engine = create_engine(\"postgresql+psycopg2://user:password@host:port/database\")\n",
    "  ```\n",
    "- Use any SQL `SELECT` query to pull data.\n",
    "- This allows flexible querying and efficient data extraction.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Modular Code Design**\n",
    "- Separate the pipeline into three main parts:\n",
    "  - **Extract**\n",
    "  - **Transform**\n",
    "  - **Load**\n",
    "- Define each step in its own function to make the code:\n",
    "  - Reusable\n",
    "  - Easier to read and maintain\n",
    "  - Aligned with the **DRY principle** (Don't Repeat Yourself)\n",
    "\n",
    "Example:\n",
    "```python\n",
    "def extract_from_parquet(path):\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "def extract_from_sql(query, conn):\n",
    "    return pd.read_sql(query, conn)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. **Time to Practice**\n",
    "- You’ll get hands-on experience with extracting data from sources like Parquet files and SQL databases.\n",
    "\n",
    "---\n",
    "\n",
    "These notes summarize how to extract data efficiently using Python and pandas, and how to structure your code for scalable pipeline development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1086264",
   "metadata": {},
   "source": [
    "### **Extracting data from parquet files**\n",
    "One of the most common ways to ingest data from a source system is by reading data from a file, such as a CSV file. As data has gotten bigger, the need for better file formats has brought about new column-oriented file types, such as parquet files.\n",
    "\n",
    "In this exercise, you'll practice extracting data from a parquet file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df5fd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the sales data into a DataFrame\n",
    "sales_data = pd.read_parquet(\"sales_data.parquet\", engine=\"fastparquet\")\n",
    "\n",
    "# Check the data type of the columns of the DataFrames\n",
    "print(sales_data.dtypes)\n",
    "\n",
    "# Print the shape of the DataFrame, as well as the head\n",
    "print(sales_data.shape)\n",
    "print(sales_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2792cc",
   "metadata": {},
   "source": [
    "### **Pulling data from SQL databases**\n",
    "SQL databases are one of the most used data storage tools in the world. Many companies have teams of several individuals responsible for creating and maintaining these databases, which typically store data crucial for day-to-day operations. These SQL databases are commonly used as source systems for a wide range of data pipelines.\n",
    "\n",
    "For this exercise, pandas has been imported as pd. Best of luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daa9e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "\n",
    "# Create a connection to the sales database\n",
    "db_engine = sqlalchemy.create_engine(\"postgresql+psycopg2://repl:password@localhost:5432/sales\")\n",
    "\n",
    "# Query the sales table\n",
    "raw_sales_data = pd.read_sql(\"select * from sales\", db_engine)\n",
    "print(raw_sales_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0444faea",
   "metadata": {},
   "source": [
    "### **Building functions to extract data**\n",
    "It's important to modularize code when building a data pipeline. This helps to make pipelines more readable and reusable, and can help to expedite troubleshooting efforts. Creating and using functions for distinct operations in a pipeline can even help when getting started on a new project by providing a framework to begin development.\n",
    "\n",
    "pandas has been imported as pd, and sqlalchemy is ready to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c8302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract():\n",
    "    connection_uri = \"postgresql+psycopg2://repl:password@localhost:5432/sales\"\n",
    "    db_engine = sqlalchemy.create_engine(connection_uri)\n",
    "    raw_data = pd.read_sql(\"SELECT * FROM sales WHERE quantity_ordered = 1\", db_engine)\n",
    "    \n",
    "    # Print the head of the DataFrame\n",
    "    print(raw_data.head())\n",
    "    \n",
    "    # Return the extracted DataFrame\n",
    "    return raw_data\n",
    "    \n",
    "# Call the extract() function\n",
    "raw_sales_data = extract()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2edf26",
   "metadata": {},
   "source": [
    "### Simplified Notes on Transforming Data with Pandas\n",
    "\n",
    "#### 1. **Importance of Data Transformation**\n",
    "- Transforming data is crucial in a data pipeline to ensure that it’s in the correct format and useful for downstream analysis.\n",
    "- Using **pandas**, data transformation becomes simple, allowing you to filter rows, create new columns, change data types, and more.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Filtering Records with `.loc[]`**\n",
    "- **`loc[]`** is a powerful pandas function to filter rows and columns based on conditions.\n",
    "  - **Example 1**: Filter rows where the `\"open\"` value is greater than zero:\n",
    "    ```python\n",
    "    df.loc[df['open'] > 0, :]\n",
    "    ```\n",
    "  - **Example 2**: Keep only specific columns, like `\"timestamps\"`, `\"open\"`, and `\"close\"`:\n",
    "    ```python\n",
    "    df.loc[:, ['timestamps', 'open', 'close']]\n",
    "    ```\n",
    "  - **Combining** conditions:\n",
    "    ```python\n",
    "    df.loc[(df['open'] > 0), ['timestamps', 'open', 'close']]\n",
    "    ```\n",
    "- **`iloc[]`** is similar but uses integer-based indexing for rows and columns.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Altering Data Types**\n",
    "- **`to_datetime()`**: Convert columns to datetime format.\n",
    "  - Example: Convert a `\"timestamps\"` column (string format) into datetime:\n",
    "    ```python\n",
    "    df['timestamps'] = pd.to_datetime(df['timestamps'], format='%Y%m%d%H%M%S')\n",
    "    ```\n",
    "  - Convert a Unix timestamp (milliseconds since 1970) to datetime:\n",
    "    ```python\n",
    "    df['timestamps'] = pd.to_datetime(df['timestamps'], unit='ms')\n",
    "    ```\n",
    "- This ensures timestamps are usable for time-based analysis.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Validating Transformations**\n",
    "- **`head()`**: Check the first few rows of the DataFrame to spot-check transformations.\n",
    "  - Example:\n",
    "    ```python\n",
    "    df.head()\n",
    "    ```\n",
    "- **`nsmallest()`** & **`nlargest()`**: Check for the smallest or largest values in columns, useful when filtering data.\n",
    "  - Example:\n",
    "    ```python\n",
    "    df['column'].nsmallest(5)\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Practice**\n",
    "- Now that you know how to filter, transform, and validate data, it’s time to apply these skills in practice exercises!\n",
    "\n",
    "---\n",
    "\n",
    "These notes summarize the essential transformation steps in a data pipeline using pandas, making it easy to filter, modify, and validate your data for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848fe613",
   "metadata": {},
   "source": [
    "### **Filtering pandas DataFrames**\n",
    "Once data has been extracted from a source system, it's time to transform it! Often, source data may have more information than what is needed for downstream use cases. If this is the case, dimensionality should be reduced during the \"transform\" phase of the data pipeline.\n",
    "\n",
    "pandas has been imported as pd, and the extract() function is available to load a DataFrame from the path that is passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbecbeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from the sales_data.parquet path\n",
    "raw_sales_data = extract(\"sales_data.parquet\")\n",
    "\n",
    "def transform(raw_data):\n",
    "  \t# Only keep rows with `Quantity Ordered` greater than 1\n",
    "    clean_data = raw_data.loc[raw_data[\"Quantity Ordered\"] > 1, :]\n",
    "\n",
    "\n",
    "    \n",
    "    # Only keep columns \"Order Date\", \"Quantity Ordered\", and \"Purchase Address\"\n",
    "    clean_data = clean_data.loc[:, [\"Order Date\", \"Quantity Ordered\", \"Purchase Address\"]]\n",
    "    \n",
    "    # Return the filtered DataFrame\n",
    "    return clean_data\n",
    "    \n",
    "transform(raw_sales_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c6e906",
   "metadata": {},
   "source": [
    "### **Transforming sales data with pandas**\n",
    "Before insights can be extracted from a dataset, column types may need to be altered to properly leverage the data. This is especially common with temporal data types, which can be stored in several different ways.\n",
    "\n",
    "For this example, pandas has been import as pd and is ready for you to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17bf45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sales_data = extract(\"sales_data.csv\")\n",
    "\n",
    "def transform(raw_data):\n",
    "    # Convert the \"Order Date\" column to type datetime\n",
    "    raw_data[\"Order Date\"] = pd.to_datetime(raw_data[\"Order Date\"], format=\"%m/%d/%y %H:%M\")\n",
    "    \n",
    "    # Only keep items under ten dollars\n",
    "    clean_data = raw_data.loc[raw_data['Price Each'] <10, :]\n",
    "    return clean_data\n",
    "\n",
    "clean_sales_data = transform(raw_sales_data)\n",
    "\n",
    "# Check the data types of each column\n",
    "print(clean_sales_data.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0e6cbe",
   "metadata": {},
   "source": [
    "### **Validating data transformations**\n",
    "Great work so far! Manually spot-checking transformations is a great first step to ensuring that you're maintaining data quality throughout a pipeline. pandas offers several built-in functions to help you with just that!\n",
    "\n",
    "To help get you started with this exercise, pandas has been imported as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fd0213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(file_path):\n",
    "    raw_data = pd.read_parquet(file_path)\n",
    "    return raw_data\n",
    "\n",
    "raw_sales_data = extract(\"sales_data.parquet\")\n",
    "\n",
    "def transform(raw_data):\n",
    "  \t# Filter rows and columns\n",
    "    clean_data = raw_data.loc[raw_data[\"Quantity Ordered\"] == 1, [\"Order ID\", \"Price Each\", \"Quantity Ordered\"]]\n",
    "    return clean_data\n",
    "\n",
    "# Transform the raw_sales_data\n",
    "clean_sales_data = transform(raw_sales_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26672ef0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 🐼 Persisting Data with Pandas — Detailed Notes\n",
    "\n",
    "### 1. **Overview**\n",
    "- After **extracting** and **transforming** data, the next step is to **load** it — typically into files or databases.\n",
    "- Pandas provides built-in methods to persist data for later use by data scientists, analysts, or downstream processes.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Importance of Persisting Data in ETL Pipelines**\n",
    "- **Persistence** means saving data at different stages (not just at the final \"Load\" step).\n",
    "- It ensures that data snapshots are available:\n",
    "  - For **recovery** in case of failure.\n",
    "  - If data is **hard to reacquire** from the original source.\n",
    "- Saves time by avoiding repeated extraction and transformation steps.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Writing DataFrames to CSV with Pandas**\n",
    "- Use `DataFrame.to_csv()` to write a DataFrame to a `.csv` file.\n",
    "- Syntax:\n",
    "  ```python\n",
    "  df.to_csv(\"path/to/filename.csv\")\n",
    "  ```\n",
    "- Example:\n",
    "  ```python\n",
    "  import pandas as pd\n",
    "  df = pd.DataFrame(data)\n",
    "  df.to_csv(\"stock_data.csv\")\n",
    "  ```\n",
    "- By default, saves:\n",
    "  - With header\n",
    "  - With index\n",
    "  - Using a comma separator\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Customizing CSV Output**\n",
    "#### a) `header` Argument:\n",
    "- Values: `True`, `False`, or list of column names (aliases).\n",
    "- Default: `True` (includes column headers).\n",
    "- Set to `False` to skip headers.\n",
    "\n",
    "#### b) `index` Argument:\n",
    "- Values: `True` (default), `False`.\n",
    "- When `True`, index column is saved.\n",
    "- Set to `False` if the index isn’t meaningful (recommended for clean output).\n",
    "\n",
    "#### c) `sep` Argument:\n",
    "- Default: `,` (comma).\n",
    "- Alternative: `|` (pipe), `\\t` (tab), etc.\n",
    "- Useful for formatting or working with special systems.\n",
    "\n",
    "#### Example:\n",
    "```python\n",
    "df.to_csv(\"file.csv\", index=False, header=True, sep=\"|\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Other File Formats in Pandas**\n",
    "Besides `to_csv`, Pandas offers:\n",
    "\n",
    "| Method         | Description                      |\n",
    "|----------------|----------------------------------|\n",
    "| `to_json()`    | Save DataFrame as JSON file      |\n",
    "| `to_excel()`   | Save as Excel `.xlsx`            |\n",
    "| `to_sql()`     | Save to a SQL database (needs connector) |\n",
    "| `to_parquet()` | Save in efficient binary format  |\n",
    "\n",
    "> These allow saving data in formats tailored for different platforms or performance needs.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Validating File Persistence with `os`**\n",
    "After writing to a file, **validate success** by checking if the file exists.\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "file_path = \"stock_data.csv\"\n",
    "if os.path.exists(file_path):\n",
    "    print(\"File saved successfully.\")\n",
    "else:\n",
    "    print(\"File not found. Something went wrong.\")\n",
    "```\n",
    "\n",
    "This check is useful for:\n",
    "- Debugging ETL pipelines.\n",
    "- Ensuring reliability in automated workflows.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Summary\n",
    "- Use `to_csv()` for quick, customizable CSV exports.\n",
    "- Persist data at **multiple ETL stages** — not just the end.\n",
    "- Validate file creation using `os.path.exists()`.\n",
    "- Explore alternatives like `to_json()`, `to_sql()`, `to_parquet()` for specific needs.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf808f1",
   "metadata": {},
   "source": [
    "### **Loading sales data to a CSV file**\n",
    "Loading data is an essential component of any data pipeline. It ensures that any data consumers and processes have reliable access to data that you've extracted and transformed earlier in a pipeline. In this exercise, you'll practice loading transformed sales data to a CSV file using pandas, which has been imported as pd. In addition to this, the raw data has been extracted and is available in the DataFrame raw_sales_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bec5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(raw_data):\n",
    "\t# Find the items prices less than 25 dollars\n",
    "\treturn raw_data.loc[raw_data[\"Price Each\"] < 25 , [\"Order ID\", \"Product\", \"Price Each\", \"Order Date\"]]\n",
    "\n",
    "def load(clean_data):\n",
    "\t# Write the data to a CSV file without the index column\n",
    "\tclean_data.to_csv(\"transformed_sales_data.csv\", index=False)\n",
    "\n",
    "\n",
    "clean_sales_data = transform(raw_sales_data)\n",
    "\n",
    "# Call the load function on the cleaned DataFrame\n",
    "load(clean_sales_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fb2d6b",
   "metadata": {},
   "source": [
    "### **Customizing a CSV file**\n",
    "Sometimes, data needs to be stored in a CSV file in a customized manner. This may include using different header values, including or excluding the index column of a DataFrame, or altering the character used to separate columns. In this example, you'll get to practice this, as well as ensuring that the file is stored in the desired file path.\n",
    "\n",
    "The pandas library has been imported as pd, and the data has already been transformed to include only rows with a \"Quantity Ordered\" greater than one. The cleaned DataFrame is stored in a variable named clean_sales_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e69882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the os library\n",
    "import os\n",
    "\n",
    "# Load the data to a csv file with the index, no header and pipe separated\n",
    "def load(clean_data, path_to_write):\n",
    "\tclean_data.to_csv(path_to_write, header=False, sep=\"|\")\n",
    "\n",
    "load(clean_sales_data, \"clean_sales_data.csv\")\n",
    "\n",
    "# Check that the file is present.\n",
    "file_exists = os.path.exists(\"clean_sales_data.csv\")\n",
    "print(file_exists)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe980e74",
   "metadata": {},
   "source": [
    "### **Persisting data to files**\n",
    "Loading data to a final destination is one of the most important steps of a data pipeline. In this exercise, you'll use the transform() function shown below to transform product sales data before loading it to a .csv file. This will give downstream data consumers a better view into total sales across a range of products.\n",
    "\n",
    "For this exercise, the sales data has been loaded and transformed, and is stored in the clean_sales_data DataFrame. The pandas package has been imported as pd, and the os library is also ready to use!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c63ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(clean_data, file_path):\n",
    "    # Write the data to a file\n",
    "    clean_data.to_csv(file_path, header= False, index=False)\n",
    "\n",
    "    # Check to make sure the file exists\n",
    "    file_exists = os.path.exists(file_path)\n",
    "    if not file_exists:\n",
    "        raise Exception(f\"File does NOT exists at path {file_path}\")\n",
    "\n",
    "# Load the transformed data to the provided file path\n",
    "load(clean_sales_data, \"transformed_sales_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410ba18b",
   "metadata": {},
   "source": [
    "Here are detailed and structured notes for the video on **Monitoring a Data Pipeline**, optimized for clarity, quick review, and practical implementation:\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Monitoring a Data Pipeline — Detailed Notes\n",
    "\n",
    "### 1. **Why Monitor Data Pipelines?**\n",
    "- Once a pipeline is built, **monitoring** ensures:\n",
    "  - Detection of **failures** during execution.\n",
    "  - Awareness of **data quality or schema changes**.\n",
    "  - Proactive **alerts** to engineers before end-users encounter issues.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Common Failures in Pipelines**\n",
    "- Missing data from source systems.\n",
    "- Schema or data type changes (e.g., string to int).\n",
    "- Deprecation or updates in tools used in the pipeline.\n",
    "\n",
    "**Goal:** Make the pipeline **transparent** and **self-alerting** to reduce manual oversight.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Using Logs to Monitor Performance**\n",
    "- **Logs** = messages recorded during execution.\n",
    "- Provide a **timeline** of what happened and when.\n",
    "- Help **debug failures** by showing the exact execution path.\n",
    "\n",
    "#### 🔧 Python’s `logging` module\n",
    "- Offers six levels of logging:\n",
    "  - `DEBUG`: Used during development to inspect internal details (e.g., shapes, values).\n",
    "  - `INFO`: Reports general progress (e.g., \"Loaded data successfully\").\n",
    "  - `WARNING`: Something unexpected happened, but code continues (e.g., unusual row count).\n",
    "  - `ERROR`: Something failed that **halts** execution (e.g., missing file or column).\n",
    "\n",
    "#### Example:\n",
    "```python\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "logging.debug(\"Data dimensions: 100 rows, 5 columns\")\n",
    "logging.info(\"Data loaded successfully\")\n",
    "logging.warning(\"Unexpected column found\")\n",
    "logging.error(\"File not found\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Capturing Warnings and Errors**\n",
    "- **Warning** logs: Notify about issues **not fatal** to execution.\n",
    "  - Example: Unexpected row count.\n",
    "- **Error** logs: Execution **must stop**.\n",
    "  - Example: Missing or corrupted data.\n",
    "\n",
    "Logging these makes it easier to:\n",
    "- **Identify root causes**\n",
    "- **Reduce troubleshooting time**\n",
    "- Improve pipeline **reliability**\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Handling Exceptions with Try-Except**\n",
    "Use `try-except` to catch and handle errors **without crashing the pipeline**.\n",
    "\n",
    "#### Basic structure:\n",
    "```python\n",
    "try:\n",
    "    # risky code\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred: {e}\")\n",
    "```\n",
    "\n",
    "#### Benefits:\n",
    "- Prevents pipeline from halting unexpectedly.\n",
    "- Ensures logs capture **why** the error happened.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Handling Specific Exceptions**\n",
    "Use **specific exception types** (like `KeyError`, `FileNotFoundError`) for better control.\n",
    "\n",
    "#### Example Use Case:\n",
    "```python\n",
    "try:\n",
    "    df = df[df[\"price_change\"] > 0]\n",
    "except KeyError as e:\n",
    "    logging.error(f\"Missing column: {e}\")\n",
    "    df[\"price_change\"] = 0\n",
    "    df = df[df[\"price_change\"] > 0]\n",
    "```\n",
    "\n",
    "- If the column `price_change` doesn't exist, log the error.\n",
    "- Create the column with a default value and re-run the filter logic.\n",
    "\n",
    "✅ This ensures pipeline continues **gracefully** and **logs the fix**.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Summary**\n",
    "| Concept | Purpose |\n",
    "|--------|---------|\n",
    "| Logging | Monitor pipeline status & performance |\n",
    "| Log Levels | Differentiate info, debug, warnings & errors |\n",
    "| Try-Except | Handle errors without crashing |\n",
    "| Specific Exceptions | Catch known issues with context |\n",
    "| Transparency | Make pipeline issues visible before users notice |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Best Practices\n",
    "- Always use **logging** in production pipelines.\n",
    "- Wrap risky operations in `try-except` blocks.\n",
    "- Log **both success and failure** events.\n",
    "- Start simple, then integrate tools like **Airflow**, **Prometheus**, or **Grafana** for advanced monitoring.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a **code template**, **Jupyter notebook**, or **alerting system suggestion** for production pipelines?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9dba03",
   "metadata": {},
   "source": [
    "### **Logging within a data pipeline**\n",
    "In this exercise, we'll take a look back at the function you wrote in a previous video and practice adding logging to the function. This will help when troubleshooting errors or making changes to the logic!\n",
    "\n",
    "pandas has been imported as pd. In addition to this, the logging module has been imported, and the default log-level has been set to \"debug\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b543a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(raw_data):\n",
    "    raw_data[\"Order Date\"] = pd.to_datetime(raw_data[\"Order Date\"], format=\"%m/%d/%y %H:%M\")\n",
    "    clean_data = raw_data.loc[raw_data[\"Price Each\"] < 10, :]\n",
    "    \n",
    "    # Create an info log regarding transformation\n",
    "    logging.info(\"Transformed 'Order Date' column to type 'datetime'.\")\n",
    "    \n",
    "    # Create debug-level logs for the DataFrame before and after filtering\n",
    "    logging.debug(f\"Shape of the DataFrame before filtering: {raw_data.shape}\")\n",
    "    logging.debug(f\"Shape of the DataFrame after filtering: {clean_data.shape}\")\n",
    "    \n",
    "    return clean_data\n",
    "  \n",
    "clean_sales_data = transform(raw_sales_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c2456e",
   "metadata": {},
   "source": [
    "### **Handling exceptions when loading data**\n",
    "Sometimes, your data pipelines might throw an exception. These exceptions are a form of alerting, and they let a Data Engineer know when something unexpected happened. It's important to properly handle these exceptions. In this exercise, we'll practice just that!\n",
    "\n",
    "To help get you started, pandas has been imported as pd, along with the logging module has been imported. The default log-level has been set to \"debug\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a2868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(file_path):\n",
    "    return pd.read_parquet(file_path)\n",
    "\n",
    "# Update the pipeline to include a try block\n",
    "try:\n",
    "\t# Attempt to read in the file\n",
    "    raw_sales_data = extract(\"sales_data.parquet\")\n",
    "\t\n",
    "# Catch the FileNotFoundError\n",
    "except FileNotFoundError as file_not_found:\n",
    "\t# Write an error-level log\n",
    "\tlogging.error(file_not_found)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6ac75b",
   "metadata": {},
   "source": [
    "### **Monitoring and alerting within a data pipeline**\n",
    "It's time to put it all together! You might have guessed it, but using handling errors using try-except and logging go hand-in-hand. These two practices are essential for a pipeline to be resilient and transparent, and are the building blocks for more advanced monitoring and alerting solutions.\n",
    "\n",
    "pandas has been imported as pd, and the logging module has been loaded and configured for you. The raw_sales_data DataFrame has been extracted, and is ready to be transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3fdf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an info-level logging message to document success, and a warning-level logging message if the transformation fails.\n",
    "def transform(raw_data):\n",
    "\treturn raw_data.loc[raw_data[\"Total Price\"] > 1000, :]\n",
    "\n",
    "try:\n",
    "\t# Attempt to transform DataFrame, log an info-level message\n",
    "\tclean_sales_data = transform(raw_sales_data)\n",
    "\tlogging.info(\"Successfully filtered DataFrame by 'Total Price'\")\n",
    "\t\n",
    "except Exception:\n",
    "\t# Log a warning-level message\n",
    "\tlogging.warning(\"Cannot filter DataFrame by 'Total Price'\")\n",
    " \n",
    " \n",
    " \n",
    "#  Update the try-except clause to catch a KeyError, and alias as ke.\n",
    "# Change the warning-level log to include the error being thrown.\n",
    "\n",
    "\n",
    "def transform(raw_data):\n",
    "\treturn raw_data.loc[raw_data[\"Total Price\"] > 1000, :]\n",
    "\n",
    "try:\n",
    "\tclean_sales_data = transform(raw_sales_data)\n",
    "\tlogging.info(\"Successfully filtered DataFrame by 'Total Price'\")\n",
    "\t\n",
    "# Update the exception to be a KeyError, alias as ke\n",
    "except KeyError as ke:\n",
    "\t# Log a warning-level message\n",
    "\tlogging.warning(f\"{ke}: Cannot filter DataFrame by 'Total Price'\")\n",
    "\n",
    "\n",
    "# If a key error is thrown, create a column \"Total Price\" by multiplying the \"Price Each\" and \"Quantity Ordered\" columns.\n",
    "def transform(raw_data):\n",
    "\treturn raw_data.loc[raw_data[\"Total Price\"] > 1000, :]\n",
    "\n",
    "try:\n",
    "\tclean_sales_data = transform(raw_sales_data)\n",
    "\tlogging.info(\"Successfully filtered DataFrame by 'Total Price'\")\n",
    "\n",
    "except KeyError as ke:\n",
    "\tlogging.warning(f\"{ke}: Cannot filter DataFrame by 'Total Price'\")\n",
    "\t\n",
    "\t# Create the \"Total Price\" column, transform the updated DataFrame\n",
    "\traw_sales_data[\"Total Price\"] = raw_sales_data[\"Price Each\"] * raw_sales_data[\"Quantity Ordered\"]\n",
    "\tclean_sales_data = transform(raw_sales_data)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
