{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be640505",
   "metadata": {},
   "source": [
    "Here are your complete and organized notes for the lesson **Manually Testing a Data Pipeline**, covering both **end-to-end validation** and **checkpoint testing**:\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Manually Testing a Data Pipeline â€” Notes\n",
    "\n",
    "Testing ensures your data pipeline extracts, transforms, and loads data reliably. Manual validation techniques help maintain **data integrity**, **performance**, and **trust** in the pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. ðŸ§ª Why Test Data Pipelines?\n",
    "\n",
    "- Ensures the pipeline behaves **as expected** before production.\n",
    "- Helps catch:\n",
    "  - Data **quality issues**\n",
    "  - **Transformation errors**\n",
    "  - **Loading failures**\n",
    "- Saves future **debugging and maintenance** time.\n",
    "- Many pipelines go untestedâ€”make yours an exception!\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ðŸŒ Environments: Testing vs Production\n",
    "\n",
    "| Testing Environment         | Production Environment          |\n",
    "|----------------------------|----------------------------------|\n",
    "| Uses **sample/mock data**  | Uses **live, production data**   |\n",
    "| Safe for **experimentation** | Must be **stable and secure**    |\n",
    "| Encourages **peer review** | Accessible by **data consumers** |\n",
    "\n",
    "> ðŸ” Always test your pipeline end-to-end in a testing environment **before** deploying to production.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ðŸ”„ End-to-End Pipeline Testing\n",
    "\n",
    "**Definition**: Simulates a full ETL run with test data.\n",
    "\n",
    "ðŸ§© Benefits:\n",
    "- Checks **pipeline reliability** over multiple runs\n",
    "- Enables **peer review** and feedback\n",
    "- Helps test **user access** to output data\n",
    "\n",
    "ðŸ› ï¸ Includes:\n",
    "- Data is extracted, transformed, and loaded fully\n",
    "- Sample data mimics structure of real production data\n",
    "- Outputs are validated at every stage\n",
    "\n",
    "---\n",
    "\n",
    "### 4. ðŸ“ Checkpoint Validation\n",
    "\n",
    "**Checkpoint** = A point **between components** in the pipeline (e.g., after extract, after transform, or after load).\n",
    "\n",
    "ðŸ‘€ Example:\n",
    "- After loading `clean_stock_data` into SQL:\n",
    "  \n",
    "```python\n",
    "loaded_data = pd.read_sql(\"SELECT * FROM clean_stock_data\", con=engine)\n",
    "\n",
    "# Check structure\n",
    "print(loaded_data.shape)\n",
    "print(loaded_data.head())\n",
    "```\n",
    "\n",
    "ðŸ“Œ Helps detect:\n",
    "- Dropped or malformed records\n",
    "- Unexpected column changes\n",
    "- Broken data types\n",
    "\n",
    "---\n",
    "\n",
    "### 5. ðŸ” DataFrame Validation\n",
    "\n",
    "Validate **output vs source** using `.equals()`:\n",
    "\n",
    "```python\n",
    "clean_stock_data.equals(loaded_data)  # Returns True or False\n",
    "```\n",
    "\n",
    "ðŸ§  This confirms:\n",
    "- **Row counts** match\n",
    "- **No data lost or changed**\n",
    "- **Schema consistency** between original and loaded tables\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Summary Table\n",
    "\n",
    "| Task                            | Method                          | Tool         |\n",
    "|---------------------------------|----------------------------------|--------------|\n",
    "| Load DataFrame from SQL         | `pd.read_sql()`                 | pandas       |\n",
    "| Quick shape/spot-check          | `.shape`, `.head()`             | pandas       |\n",
    "| Full equality check             | `.equals(other_df)`             | pandas       |\n",
    "| End-to-end pipeline test        | Simulate full ETL               | Custom code  |\n",
    "| Peer review + consumer testing  | Share test data results         | Collaboration |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸš€ Final Tip\n",
    "\n",
    "Always **test in stages** (checkpoints), and also do a **full run** (end-to-end). This ensures both granular and holistic reliability before deploying a pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like a test script template for validating checkpoints or comparing DataFrames!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28438094",
   "metadata": {},
   "source": [
    "### **Validating a data pipeline at \"checkpoints\"**\n",
    "In this exercise, you'll be working with a data pipeline that extracts tax data from a CSV file, creates a new column, filters out rows based on average taxable income, and persists the data to a parquet file.\n",
    "\n",
    "pandas has been loaded as pd, and the extract(), transform(), and load() functions have already been defined. You'll use these functions to validate the data pipeline at various checkpoints throughout its execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f603a971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and transform tax_data\n",
    "raw_tax_data = extract(\"raw_tax_data.csv\")\n",
    "clean_tax_data = transform(raw_tax_data)\n",
    "\n",
    "# Check the shape of the raw_tax_data DataFrame, compare to the clean_tax_data DataFrame\n",
    "print(f\"Shape of raw_tax_data: {raw_tax_data.shape}\")\n",
    "print(f\"Shape of clean_tax_data: {clean_tax_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9748d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tax_data = extract(\"raw_tax_data.csv\")\n",
    "clean_tax_data = transform(raw_tax_data)\n",
    "load(clean_tax_data, \"clean_tax_data.parquet\")\n",
    "\n",
    "print(f\"Shape of raw_tax_data: {raw_tax_data.shape}\")\n",
    "print(f\"Shape of clean_tax_data: {clean_tax_data.shape}\")\n",
    "\n",
    "# Read in the loaded data, observe the head of each\n",
    "to_validate = pd.read_parquet(\"clean_tax_data.parquet\")\n",
    "print(clean_tax_data.head(3))\n",
    "print(to_validate.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5482556a",
   "metadata": {},
   "source": [
    "### **Validating a data pipeline at \"checkpoints\"**\n",
    "In this exercise, you'll be working with a data pipeline that extracts tax data from a CSV file, creates a new column, filters out rows based on average taxable income, and persists the data to a parquet file.\n",
    "\n",
    "pandas has been loaded as pd, and the extract(), transform(), and load() functions have already been defined. You'll use these functions to validate the data pipeline at various checkpoints throughout its execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5746c8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_tax_data = extract(\"raw_tax_data.csv\")\n",
    "clean_tax_data = transform(raw_tax_data)\n",
    "load(clean_tax_data, \"clean_tax_data.parquet\")\n",
    "\n",
    "print(f\"Shape of raw_tax_data: {raw_tax_data.shape}\")\n",
    "print(f\"Shape of clean_tax_data: {clean_tax_data.shape}\")\n",
    "\n",
    "to_validate = pd.read_parquet(\"clean_tax_data.parquet\")\n",
    "print(clean_tax_data.head(3))\n",
    "print(to_validate.head(3))\n",
    "\n",
    "# Check that the DataFrames are equal\n",
    "print(to_validate.equals(clean_tax_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac995d56",
   "metadata": {},
   "source": [
    "### **Testing a data pipeline end-to-end**\n",
    "In this exercise, you'll be working with the same data pipeline as before, which extracts, transforms, and loads tax data. You'll practice testing this pipeline end-to-end to ensure the solution can be run multiple times, without duplicating the transformed data in the parquet file.\n",
    "\n",
    "pandas has been loaded as pd, and the extract(), transform(), and load() functions have already been defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7a876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger the data pipeline to run three times\n",
    "for attempt in range(0, 3):\n",
    "\tprint(f\"Attempt: {attempt}\")\n",
    "\traw_tax_data = extract(\"raw_tax_data.csv\")\n",
    "\tclean_tax_data = transform(raw_tax_data)\n",
    "\tload(clean_tax_data, \"clean_tax_data.parquet\")\n",
    "\t\n",
    "\t# Print the shape of the cleaned_tax_data DataFrame\n",
    "\tprint(f\"Shape of clean_tax_data: {clean_tax_data.shape}\")\n",
    "    \n",
    "# Read in the loaded data, check the shape\n",
    "to_validate = pd.read_parquet(\"clean_tax_data.parquet\")\n",
    "print(f\"Final shape of cleaned data: {to_validate.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803e63a3",
   "metadata": {},
   "source": [
    "Here are your detailed and structured notes for the lesson **Unit Testing a Data Pipeline**, including key concepts, code examples, and practical takeaways:\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Unit Testing a Data Pipeline â€” Notes\n",
    "\n",
    "**Unit tests** are small, fast, and repeatable tests that ensure each part of your pipeline behaves correctly **in isolation**. They're a foundational tool for **robust, error-resistant pipelines**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. ðŸ§ª What are Unit Tests?\n",
    "\n",
    "- Code that **tests other code** for correctness.\n",
    "- Common in **software engineering**, and equally powerful in **data engineering**.\n",
    "- Should be written **before end-to-end** validation.\n",
    "- Can test both:\n",
    "  - **Functionality** of code\n",
    "  - **Quality/structure** of data\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ðŸ”§ Unit Testing with `pytest`\n",
    "\n",
    "- `pytest` is a Python library used to write and run unit tests.\n",
    "- **Test functions** should start with `test_` to be discovered automatically by `pytest`.\n",
    "\n",
    "ðŸ§ª Example:\n",
    "\n",
    "```python\n",
    "def test_transformed_data():\n",
    "    assert isinstance(clean_stock_data, pd.DataFrame)\n",
    "```\n",
    "\n",
    "â–¶ï¸ Run the test:\n",
    "```bash\n",
    "python -m pytest\n",
    "```\n",
    "\n",
    "If the assertion passes, the test is âœ… successful.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. âœ… `assert` and `isinstance` Basics\n",
    "\n",
    "```python\n",
    "pipeline_type = \"ETL\"\n",
    "\n",
    "# Check type\n",
    "assert isinstance(pipeline_type, str)  # âœ…\n",
    "\n",
    "# Check value\n",
    "assert pipeline_type == \"ETL\"  # âœ…\n",
    "\n",
    "# Failing example:\n",
    "assert isinstance(pipeline_type, float)  # âŒ Raises AssertionError\n",
    "```\n",
    "\n",
    "- `assert`: Raises an error if the condition is **False**\n",
    "- `isinstance(obj, type)`: Checks if `obj` is an instance of `type`\n",
    "\n",
    "---\n",
    "\n",
    "### 4. ðŸ§ª Pytest Fixtures for Test Data\n",
    "\n",
    "Fixtures help:\n",
    "- **Reuse setup code**\n",
    "- Provide **test data or objects**\n",
    "- Keep tests **clean and readable**\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def clean_data():\n",
    "    return pd.DataFrame({\n",
    "        \"open\": [100.5, 110.3],\n",
    "        \"close\": [105.2, 115.0],\n",
    "        \"volume\": [1000, 1200],\n",
    "        \"symbol\": [\"AAPL\", \"MSFT\"]\n",
    "    })\n",
    "\n",
    "def test_transformed_data(clean_data):\n",
    "    assert isinstance(clean_data, pd.DataFrame)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. ðŸ“Š Unit Testing DataFrames\n",
    "\n",
    "You can **test the data itself**, not just the functions!\n",
    "\n",
    "âœ… Example tests:\n",
    "\n",
    "```python\n",
    "def test_column_count(clean_data):\n",
    "    assert len(clean_data.columns) == 4\n",
    "\n",
    "def test_open_column_positive(clean_data):\n",
    "    assert (clean_data[\"open\"] > 0).all()\n",
    "\n",
    "def test_open_column_max(clean_data):\n",
    "    assert clean_data[\"open\"].max() < 200\n",
    "```\n",
    "\n",
    "ðŸ§  Use built-in pandas methods like `.min()`, `.max()`, `.columns`, and `.shape`.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Summary Table\n",
    "\n",
    "| Concept                     | Tool/Method           | Example                              |\n",
    "|----------------------------|------------------------|--------------------------------------|\n",
    "| Unit test                  | `def test_...()`       | `def test_transformed_data()`        |\n",
    "| Run all tests              | CLI                    | `python -m pytest`                   |\n",
    "| Type check                 | `assert isinstance()`  | `assert isinstance(df, pd.DataFrame)`|\n",
    "| Fixtures (reusable data)   | `@pytest.fixture`      | `def clean_data()`                   |\n",
    "| Data validation            | Pandas assertions      | `assert (df[\"open\"] > 0).all()`      |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸš€ Final Tip\n",
    "\n",
    "Think of unit testing as building a **safety net**. It ensures small changes in your pipeline don't cause unexpected, costly errors downstream.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like a **unit test template** or want help writing test cases for your own pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71241177",
   "metadata": {},
   "source": [
    "### **Validating a data pipeline with assert**\n",
    "To build unit tests for data pipelines, it's important to get familiar with the assert keyword, and the isinstance() function. In this exercise, you'll practice using these two tools to validate components of a data pipeline.\n",
    "\n",
    "The functions extract() and transform() have been made available for you, along with pandas, which has been imported as pd. Both extract() and transform() return a DataFrame. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dfdb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tax_data = extract(\"raw_tax_data.csv\")\n",
    "clean_tax_data = transform(raw_tax_data)\n",
    "\n",
    "# Validate the number of columns in the DataFrame\n",
    "assert len(clean_tax_data.columns) == 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e99570",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tax_data = extract(\"raw_tax_data.csv\")\n",
    "clean_tax_data = transform(raw_tax_data)\n",
    "\n",
    "# Determine if the clean_tax_data DataFrames take type pd.DataFrame\n",
    "assert isinstance(clean_tax_data, pd.DataFrame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb74e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tax_data = extract(\"raw_tax_data.csv\")\n",
    "clean_tax_data = transform(raw_tax_data)\n",
    "\n",
    "# Assert that clean_tax_data takes is an instance of a string\n",
    "try:\n",
    " assert isinstance(clean_tax_data, str)\n",
    "except Exception as e:\n",
    "\tprint(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb668fc",
   "metadata": {},
   "source": [
    "\n",
    "### **Writing unit tests with pytest**\n",
    "In this exercise, you'll practice writing a unit test to validate a data pipeline. You'll use assert and other tools to build the tests, and determine if the data pipeline performs as it should.\n",
    "\n",
    "The functions extract() and transform() have been made available for you, along with pandas, which has been imported as pd. You'll be testing the transform() function, which is shown below.\n",
    "```python\n",
    "def transform(raw_data):\n",
    "    raw_data[\"average_taxable_income\"] = raw_data[\"total_taxable_income\"] / raw_data[\"number_of_firms\"]\n",
    "    clean_data = raw_data.loc[raw_data[\"average_taxable_income\"] > 100, :]\n",
    "    clean_data.set_index(\"industry_name\", inplace=True)\n",
    "    return clean_data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d38853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "def test_transformed_data():\n",
    "    raw_tax_data = extract(\"raw_tax_data.csv\")\n",
    "    clean_tax_data = transform(raw_tax_data)\n",
    "    \n",
    "    # Assert that the transform function returns a pd.DataFrame\n",
    "    assert isinstance(clean_tax_data, pd.DataFrame)\n",
    "    \n",
    "    # Assert that the clean_tax_data DataFrame has more columns than the raw_tax_data DataFrame\n",
    "    assert len(clean_tax_data.columns) > len(raw_tax_data.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9280dcc",
   "metadata": {},
   "source": [
    "Here are your detailed, structured notes for the lesson **\"Running a Data Pipeline in Production\"**, covering architecture, execution, orchestration, and monitoring best practices.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Running a Data Pipeline in Production â€” Notes\n",
    "\n",
    "This lesson ties everything together, showing how to **structure, execute, and scale** a data pipeline for production environments.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. ðŸ—ï¸ Data Pipeline Architecture Patterns\n",
    "\n",
    "| Approach                        | Description |\n",
    "|-------------------------------|-------------|\n",
    "| **Single script file**         | Easy but cluttered. All ETL logic and execution code in one file. |\n",
    "| **Modular architecture** âœ…    | Separate **function definitions** and **execution logic** into separate files for clarity and reusability. |\n",
    "\n",
    "**Better structure example:**\n",
    "\n",
    "```bash\n",
    "project/\n",
    "â”‚\n",
    "â”œâ”€â”€ pipeline_utils.py     # extract(), transform(), load()\n",
    "â”œâ”€â”€ run_pipeline.py       # imports and runs ETL using try/except\n",
    "```\n",
    "\n",
    "In `run_pipeline.py`:\n",
    "\n",
    "```python\n",
    "from pipeline_utils import extract, transform, load\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "try:\n",
    "    data = extract()\n",
    "    clean_data = transform(data)\n",
    "    load(clean_data)\n",
    "    logging.info(\"Pipeline ran successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Pipeline failed: {e}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ðŸ” End-to-End Pipeline Execution\n",
    "\n",
    "- Use **`try/except`** blocks for error handling\n",
    "- Use **`logging`** to monitor pipeline status\n",
    "- Run pipeline via CLI or scheduler in production\n",
    "\n",
    "ðŸ”§ This setup:\n",
    "- Is **simple**, yet powerful\n",
    "- Gives a **basic monitoring & alerting** setup\n",
    "- Works as the base for production deployment\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ðŸ§© Orchestration Tools for Production\n",
    "\n",
    "#### ðŸ’¡ Why Use Orchestration Tools?\n",
    "\n",
    "| Feature                     | Description |\n",
    "|----------------------------|-------------|\n",
    "| â° **Scheduling**            | Run tasks at intervals (daily, hourly, etc.) |\n",
    "| ðŸ” **Retry on failure**     | Automatically re-run failed tasks |\n",
    "| ðŸš¨ **Alerting**             | Notify on errors (Slack, email, etc.) |\n",
    "| âš–ï¸ **Resource scaling**     | Efficiently manage compute resources |\n",
    "| ðŸ“ˆ **Monitoring UI**        | Visual dashboards for pipeline status |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. ðŸ› ï¸ Popular Orchestration Tools\n",
    "\n",
    "| Tool        | Notes |\n",
    "|-------------|-------|\n",
    "| **Apache Airflow** ðŸ† | Most popular (~40% usage). Open-source, rich UI, plugin support. |\n",
    "| **Prefect**          | Pythonic, modern, easier to configure than Airflow. |\n",
    "| **Dagster**          | Emphasizes data quality and observability. |\n",
    "\n",
    "> ðŸ’¡ Source: [State of Data Engineering â€“ Seattle Data Guy](https://open.substack.com/pub/seattledataguy/p/the-state-of-data-engineering-part)\n",
    "\n",
    "---\n",
    "\n",
    "### 5. âš™ï¸ Airflow Example (Basic Idea)\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from datetime import datetime\n",
    "\n",
    "def extract(): ...\n",
    "def transform(): ...\n",
    "def load(): ...\n",
    "\n",
    "dag = DAG('my_etl_pipeline', start_date=datetime(2024, 1, 1), schedule_interval='@daily')\n",
    "\n",
    "task1 = PythonOperator(task_id='extract', python_callable=extract, dag=dag)\n",
    "task2 = PythonOperator(task_id='transform', python_callable=transform, dag=dag)\n",
    "task3 = PythonOperator(task_id='load', python_callable=load, dag=dag)\n",
    "\n",
    "task1 >> task2 >> task3\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”š Summary Table\n",
    "\n",
    "| Key Concept                | Best Practice                            |\n",
    "|---------------------------|-------------------------------------------|\n",
    "| Code organization         | Separate logic (`pipeline_utils.py`) from execution (`run_pipeline.py`) |\n",
    "| Logging and error handling| Use `try/except` and `logging`            |\n",
    "| Orchestration             | Use Airflow, Prefect, or Dagster for scheduling, retries, alerting |\n",
    "| Manual execution          | Via Python script (`python run_pipeline.py`) |\n",
    "| Monitoring                | Set up alerts/logs or use orchestrator UI |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ¨ Final Tip\n",
    "\n",
    "When moving to production:\n",
    "- **Automate everything**\n",
    "- **Fail gracefully**\n",
    "- **Log everything**\n",
    "- **Monitor & alert proactively**\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want a **sample folder structure** or a ready-made Airflow DAG template to test out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d675c632",
   "metadata": {},
   "source": [
    "### **Data pipeline architecture patterns**\n",
    "When building data pipelines, it's best to separate the files where functions are being defined from where they are being run.\n",
    "\n",
    "In this exercise, you'll practice importing components of a pipeline into memory before using these functions to run the pipeline end-to-end. The project takes the following format, where pipeline_utils stores the extract(), transform(), and load() functions that will be used run the pipeline.\n",
    "\n",
    "```bash\n",
    " > ls\n",
    " etl_pipeline.py\n",
    " pipeline_utils.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a27077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the extract, transform, and load functions from pipeline_utils\n",
    "from pipeline_utils import extract, transform, load\n",
    "\n",
    "# Run the pipeline end to end by extracting, transforming and loading the data\n",
    "raw_tax_data = extract(\"raw_tax_data.csv\")\n",
    "clean_tax_data = transform(raw_tax_data)\n",
    "load(clean_tax_data, \"clean_tax_data.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a469022",
   "metadata": {},
   "source": [
    "### **Running a data pipeline end-to-end**\n",
    "It's important to monitor the performance of a pipeline when running in production. Earlier in the course, you explored tools such as exception handling and logging. In this last exercise, we'll practice running a pipeline end-to-end, while monitoring for exceptions and logging performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3270bf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pipeline_utils import extract, transform, load\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s: %(message)s', level=logging.DEBUG)\n",
    "\n",
    "try:\n",
    "\traw_tax_data = extract(\"raw_tax_data.csv\")\n",
    "\tclean_tax_data = transform(raw_tax_data)\n",
    "\tload(clean_tax_data, \"clean_tax_data.parquet\")\n",
    "    \n",
    "\tlogging.info(\"Successfully extracted, transformed and loaded data.\")  # Log a success message.\n",
    "    \n",
    "except Exception as e:\n",
    "\tlogging.error(f\"Pipeline failed with error: {e}\")  # Log failure message\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
