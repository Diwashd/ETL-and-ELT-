{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25d45af6",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üß© Extracting Non-Tabular Data ‚Äî Detailed Notes\n",
    "\n",
    "### 1. üì¶ Transitioning from Tabular to Non-Tabular Data\n",
    "- Previous work focused on **tabular data** (structured: rows and columns).\n",
    "- In this lesson, the focus shifts to **non-tabular data** ‚Äî the majority in real-world scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. üåê Understanding Non-Tabular Data\n",
    "\n",
    "> **Fact**: Over 80% of data produced today is **unstructured** (source: MIT Sloan).\n",
    "\n",
    "#### Types of Non-Tabular Data:\n",
    "| Type       | Examples |\n",
    "|------------|----------|\n",
    "| Text       | Emails, Social Media, Logs |\n",
    "| Audio      | Voice commands, Podcasts |\n",
    "| Image      | Photos, Diagrams |\n",
    "| Video      | Surveillance, Tutorials |\n",
    "| Spatial    | GIS data, Maps |\n",
    "| IoT Data   | Sensor readings, Device logs |\n",
    "\n",
    "üîç **Goal of the Data Engineer**: Convert raw unstructured data into a **structured/tabular format** for analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. üîå Working with APIs and JSON Data\n",
    "\n",
    "#### What is an API?\n",
    "- **API (Application Programming Interface)** = Interface to interact with software/data without direct DB access.\n",
    "- Similar to a **bank teller** ‚Äì you can deposit or withdraw, but not access the vault.\n",
    "\n",
    "#### Key Features:\n",
    "- Common in third-party data extraction.\n",
    "- Ensures **security** (no direct DB access).\n",
    "- Most APIs return data in **JSON format**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. üìÅ What is JSON?\n",
    "\n",
    "- **JSON (JavaScript Object Notation)** = Schema-less, key-value structure.\n",
    "- Resembles a **Python dictionary**.\n",
    "- Can store:\n",
    "  - Simple key-value pairs.\n",
    "  - Lists.\n",
    "  - Nested dictionaries (can get complex!).\n",
    "\n",
    "---\n",
    "\n",
    "### 5. üìö Reading JSON with `pandas.read_json()`\n",
    "\n",
    "Use this when JSON is **already in a structured format**.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.read_json(\"path_to_file.json\", orient=\"columns\")\n",
    "```\n",
    "\n",
    "#### `orient` Parameter:\n",
    "| Orient Value | JSON Structure                                      |\n",
    "|--------------|-----------------------------------------------------|\n",
    "| `\"columns\"`  | `{col1: [..], col2: [..]}` (default)               |\n",
    "| `\"records\"`  | `[{col1: val1, col2: val2}, {...}]`               |\n",
    "| `\"index\"`    | `{index1: {col1: val, col2: val}, ...}`           |\n",
    "\n",
    "üîé Use the right `orient` value based on how your JSON file is structured.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. üß¨ Dealing with Nested or Complex JSON\n",
    "\n",
    "- If JSON is **not flat** or **DataFrame-ready**:\n",
    "  - Load it as a **dictionary** first using Python's `json` module.\n",
    "  - Then manually transform into DataFrame.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. üêç Reading JSON Using `json.load()`\n",
    "\n",
    "```python\n",
    "import json\n",
    "\n",
    "with open(\"file.json\") as file:\n",
    "    raw_data = json.load(file)  # returns dict or list\n",
    "```\n",
    "\n",
    "- Returns Python dict (or list).\n",
    "- You can inspect with `type(raw_data)` and start transforming:\n",
    "  - Extract nested fields.\n",
    "  - Flatten structures.\n",
    "  - Convert list of dicts to DataFrame.\n",
    "\n",
    "Example transformation:\n",
    "```python\n",
    "df = pd.DataFrame(raw_data[\"data\"])  # if \"data\" key holds records\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 8. ‚úÖ Summary of Best Practices\n",
    "\n",
    "| Task | Tool | Notes |\n",
    "|------|------|-------|\n",
    "| Structured JSON | `pd.read_json()` | Use `orient` wisely |\n",
    "| Unstructured/Nested JSON | `json.load()` | Flatten to dict ‚Üí transform to DataFrame |\n",
    "| Real-time data/API | Requests + JSON | Use `.json()` method to parse response |\n",
    "| Goal | Transform non-tabular ‚Üí tabular | Enables analysis |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like code samples for **flattening deeply nested JSON**, or examples of **real-world API extraction** using Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8e8364",
   "metadata": {},
   "source": [
    "### **Ingesting JSON data with pandas**\n",
    "When developing a data pipeline, you may have to work with non-tabular data and data sources, such as APIs or JSON files. In this exercise, we'll practice extracting data from a JSON file using pandas.\n",
    "\n",
    "pandas has been imported as pd, and the JSON file you'll ingest is stored at the path \"testing_scores.json\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12c9ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(file_path):\n",
    "  # Read the JSON file into a DataFrame\n",
    "  return pd.read_json(file_path, orient=\"records\")\n",
    "\n",
    "# Call the extract function with the appropriate path, assign to raw_testing_scores\n",
    "raw_testing_scores = extract(\"testing_scores.json\")\n",
    "\n",
    "\n",
    "# Output the head of the DataFrame\n",
    "print(raw_testing_scores.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e651cf",
   "metadata": {},
   "source": [
    "### **Reading JSON data into memory**\n",
    "When data is stored in JSON format, it's not always easy to load into a DataFrame. This is the case for the \"nested_testing_scores.json\" file. Here, the data will have to be manually manipulated before it can be stored in a DataFrame.\n",
    "\n",
    "To help get you started, pandas has been loaded into the workspace as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee3611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to read a JSON file into a DataFrame.\n",
    "# Pass the \"nested_scores.json\" file path to the extract() function.\n",
    "\n",
    "def extract(file_path):\n",
    "  \t# Read the JSON file into a DataFrame, orient by index\n",
    "\treturn pd.read_json(file_path, orient=\"index\")\n",
    "\n",
    "# Call the extract function, pass in the desired file_path\n",
    "raw_testing_scores = extract(\"nested_scores.json\")\n",
    "print(raw_testing_scores.head())\n",
    "\n",
    "\n",
    "# Import the json library.\n",
    "# Use the json library to load the \"nested_scores.json\" file into memory.\n",
    "# Import the json library\n",
    "import json\n",
    "\n",
    "def extract(file_path):\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        # Load the data from the JSON file\n",
    "        raw_data = json.load(json_file)\n",
    "    return raw_data\n",
    "\n",
    "raw_testing_scores = extract(\"nested_scores.json\")\n",
    "\n",
    "# Print the raw_testing_scores\n",
    "print(raw_testing_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40d697a",
   "metadata": {},
   "source": [
    "Here are detailed and structured notes on the topic **Transforming Non-Tabular Data**, especially focusing on converting nested JSON or dictionary structures into a pandas DataFrame ‚Äî a vital skill for data pipeline construction:\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Transforming Non-Tabular Data ‚Äî Detailed Notes\n",
    "\n",
    "### 1. üó∫Ô∏è Recap: From JSON to Dictionary\n",
    "- **Previous step**: Parsed complex JSON using `json.load()` into a Python dictionary.\n",
    "- ‚úÖ Now the task is to **transform this dictionary into a tabular structure (DataFrame)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. üß± Dictionary Structure Recap\n",
    "\n",
    "Typical structure of the loaded dictionary:\n",
    "```python\n",
    "{\n",
    "  \"2023-01-01\": {\n",
    "    \"price\": {\"open\": 150, \"close\": 158},\n",
    "    \"volume\": 30000\n",
    "  },\n",
    "  \"2023-01-02\": {\n",
    "    \"price\": {\"open\": 160, \"close\": 165},\n",
    "    \"volume\": 32000\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "- üìå Keys: Timestamps (or unique IDs).\n",
    "- üìå Values: Nested dictionaries (with price, volume, etc.).\n",
    "- üìå Target: Convert this structure into a **list of lists** ‚Üí DataFrame.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. üîÅ Iterating Over Dictionary Components\n",
    "\n",
    "#### Dictionary methods:\n",
    "| Method     | Output Type  | Description |\n",
    "|------------|--------------|-------------|\n",
    "| `.keys()`  | list         | List of all keys (e.g. timestamps) |\n",
    "| `.values()`| list         | List of values (e.g. nested dicts) |\n",
    "| `.items()` | list of tuples | Each tuple = (key, value) pair |\n",
    "\n",
    "‚úÖ Use `.items()` when both key and value are needed during iteration.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. üîç Accessing Dictionary Values with `.get()`\n",
    "\n",
    "```python\n",
    "value = dictionary.get(\"key\", default_value)\n",
    "```\n",
    "\n",
    "#### Benefits:\n",
    "- Avoids errors if a key is missing.\n",
    "- You can specify a fallback value (`None` or something else).\n",
    "- ‚úÖ Use `.get()` **twice** for nested dictionaries.\n",
    "\n",
    "Example (nested access):\n",
    "```python\n",
    "open_price = value.get(\"price\", {}).get(\"open\", None)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. üß± Transforming into List of Lists\n",
    "\n",
    "Create a list where each element is a **row**:\n",
    "```python\n",
    "parsed_stock_data = []\n",
    "\n",
    "for timestamp, data in raw_stock_data.items():\n",
    "    open_price = data.get(\"price\", {}).get(\"open\", None)\n",
    "    close_price = data.get(\"price\", {}).get(\"close\", None)\n",
    "    volume = data.get(\"volume\", None)\n",
    "    \n",
    "    parsed_stock_data.append([timestamp, open_price, close_price, volume])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. üìä Creating a DataFrame\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(parsed_stock_data, columns=[\"timestamp\", \"open\", \"close\", \"volume\"])\n",
    "df.set_index(\"timestamp\", inplace=True)\n",
    "```\n",
    "\n",
    "> üß† Now your non-tabular JSON data has become a clean, tabular DataFrame!\n",
    "\n",
    "---\n",
    "\n",
    "### 7. ‚úÖ Summary Workflow\n",
    "\n",
    "| Step | Action |\n",
    "|------|--------|\n",
    "| 1Ô∏è‚É£ | Load JSON using `json.load()` |\n",
    "| 2Ô∏è‚É£ | Use `.items()` to loop through key-value pairs |\n",
    "| 3Ô∏è‚É£ | Use `.get()` for safe value access |\n",
    "| 4Ô∏è‚É£ | Build a `list of lists` with the desired fields |\n",
    "| 5Ô∏è‚É£ | Convert to DataFrame using `pd.DataFrame()` |\n",
    "| 6Ô∏è‚É£ | Set meaningful column names and index |\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Example Code (All Together)\n",
    "```python\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load JSON data\n",
    "with open(\"stock_data.json\") as file:\n",
    "    raw_stock_data = json.load(file)\n",
    "\n",
    "# Step 2: Transform dictionary into list of lists\n",
    "parsed_stock_data = []\n",
    "for timestamp, data in raw_stock_data.items():\n",
    "    open_price = data.get(\"price\", {}).get(\"open\", None)\n",
    "    close_price = data.get(\"price\", {}).get(\"close\", None)\n",
    "    volume = data.get(\"volume\", None)\n",
    "    parsed_stock_data.append([timestamp, open_price, close_price, volume])\n",
    "\n",
    "# Step 3: Create DataFrame\n",
    "df = pd.DataFrame(parsed_stock_data, columns=[\"timestamp\", \"open\", \"close\", \"volume\"])\n",
    "df.set_index(\"timestamp\", inplace=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Would you like to try transforming **another nested JSON structure**, or need an example with real-world APIs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9f8fb6",
   "metadata": {},
   "source": [
    "### **Iterating over dictionaries**\n",
    "Once JSON data is loaded into a dictionary, you can leverage Python's built-in tools to iterate over its keys and values.\n",
    "\n",
    "The \"nested_school_scores.json\" file has been read into a dictionary stored in the raw_testing_scores variable, which takes the following form:\n",
    "```\n",
    "{\n",
    "    \"01M539\": {\n",
    "        \"street_address\": \"111 Columbia Street\",\n",
    "        \"city\": \"Manhattan\",\n",
    "        \"scores\": {\n",
    "              \"math\": 657,\n",
    "              \"reading\": 601,\n",
    "              \"writing\": 601\n",
    "        }\n",
    "  }, ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077130f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_testing_scores_keys = []\n",
    "raw_testing_scores_values = []\n",
    "\n",
    "# Iterate through the values of the raw_testing_scores dictionary\n",
    "for school_id, school_info in raw_testing_scores.items():\n",
    "\traw_testing_scores_keys.append(school_id)\n",
    "\traw_testing_scores_values.append(school_info)\n",
    "\n",
    "print(raw_testing_scores_keys[0:3])\n",
    "print(raw_testing_scores_values[0:3])\n",
    "\n",
    "\n",
    "\n",
    "# Iterate through the values of the raw_testing_scores dictionary\n",
    "for school_info in raw_testing_scores.values():\n",
    "\traw_testing_scores_values.append(school_info)\n",
    "    \n",
    "print(raw_testing_scores_values[0:3])\n",
    "\n",
    "raw_testing_scores_keys = []\n",
    "raw_testing_scores_values = []\n",
    "\n",
    "# Iterate through the values of the raw_testing_scores dictionary\n",
    "for school_id, school_info in raw_testing_scores.items():\n",
    "\traw_testing_scores_keys.append(school_id)\n",
    "\traw_testing_scores_values.append(school_info)\n",
    "\n",
    "print(raw_testing_scores_keys[0:3])\n",
    "print(raw_testing_scores_values[0:3])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab626b5",
   "metadata": {},
   "source": [
    "### **Parsing data from dictionaries**\n",
    "When JSON data is loaded into memory, the resulting dictionary can be complicated. Key-value pairs may contain another dictionary, such are called nested dictionaries. These nested dictionaries are frequently encountered when dealing with APIs or other JSON data. In this exercise, you will practice extracting data from nested dictionaries and handling missing values.\n",
    "\n",
    "The dictionary below is stored in the school variable. Good luck!\n",
    "```\n",
    "{\n",
    "    \"street_address\": \"111 Columbia Street\",\n",
    "    \"city\": \"Manhattan\",\n",
    "    \"scores\": {\n",
    "        \"math\": 657,\n",
    "        \"reading\": 601\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e8bee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the street_address from the dictionary\n",
    "street_address = school.get(\"street_address\")\n",
    "\n",
    "# Parse the scores dictionary\n",
    "scores = school.get(\"scores\")\n",
    "\n",
    "# Try to parse the math, reading and writing values from scores\n",
    "math_score = scores.get(\"math\", 0)\n",
    "reading_score = scores.get('reading',0)\n",
    "writing_score = scores.get('writing',0)\n",
    "\n",
    "print(f\"Street Address: {street_address}\")\n",
    "print(f\"Math: {math_score}, Reading: {reading_score}, Writing: {writing_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9c4480",
   "metadata": {},
   "source": [
    "### **Transforming JSON data**\n",
    "Chances are, when reading data from JSON format into a dictionary, you'll probably have to apply some level of manual transformation to the data before it can be stored in a DataFrame. This is common when working with nested dictionaries, which you'll have the opportunity to explore in this exercise.\n",
    "\n",
    "The \"nested_school_scores.json\" file has been read into a dictionary available in the raw_testing_scores variable, which takes the following form:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"01M539\": {\n",
    "        \"street_address\": \"111 Columbia Street\",\n",
    "        \"city\": \"Manhattan\",\n",
    "        \"scores\": {\n",
    "              \"math\": 657,\n",
    "              \"reading\": 601,\n",
    "              \"writing\": 601\n",
    "        }\n",
    "  }, ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f51dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_testing_scores = []\n",
    "\n",
    "# Loop through each of the dictionary key-value pairs\n",
    "for school_id, school_info in raw_testing_scores.items():\n",
    "\tnormalized_testing_scores.append([\n",
    "    \tschool_id,\n",
    "    \tschool_info.get(\"street_address\"),  # Pull the \"street_address\"\n",
    "    \tschool_info.get(\"city\"),\n",
    "    \tschool_info.get(\"scores\").get(\"math\", 0),\n",
    "    \tschool_info.get(\"scores\").get(\"reading\", 0),\n",
    "    \tschool_info.get(\"scores\").get(\"writing\", 0),\n",
    "    ])\n",
    "\n",
    "print(normalized_testing_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55723b02",
   "metadata": {},
   "source": [
    "### **Transforming and cleaning DataFrames**\n",
    "Once data has been curated into a cleaned Python data structure, such as a list of lists, it's easy to convert this into a pandas DataFrame. You'll practice doing just this with the data that was curated in the last exercise.\n",
    "\n",
    "Per usual, pandas has been imported as pd, and the normalized_testing_scores variable stores the list of each schools testing data, as shown below.\n",
    "```\n",
    "[\n",
    "    ['01M539', '111 Columbia Street', 'Manhattan', 657.0, 601.0, 601.0],\n",
    "    ...\n",
    "]   \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b433529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the normalized_testing_scores list\n",
    "normalized_data =  pd.DataFrame(normalized_testing_scores)\n",
    "\n",
    "# Set the column names\n",
    "normalized_data.columns = [\"school_id\", \"street_address\", \"city\", \"avg_score_math\", \"avg_score_reading\", \"avg_score_writing\"]\n",
    "\n",
    "normalized_data = normalized_data.set_index(\"school_id\")\n",
    "print(normalized_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f7ac48",
   "metadata": {},
   "source": [
    "Here are detailed and well-structured notes for **Advanced Data Transformation with Pandas**, a crucial step for refining data within pipelines:\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Advanced Data Transformation with pandas ‚Äî Notes\n",
    "\n",
    "After converting non-tabular data to a DataFrame, the next step is **cleaning, organizing, and enriching the data** using pandas‚Äô advanced functionalities.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. ü©π Handling Missing Values (`.fillna()`)\n",
    "\n",
    "#### üîπ Basic usage:\n",
    "```python\n",
    "df.fillna(0)\n",
    "```\n",
    "Replaces **all NaN values** in the DataFrame with `0`.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ Column-specific replacement using a dictionary:\n",
    "```python\n",
    "df.fillna(value={\"open\": 0, \"close\": 0.5}, axis=1)\n",
    "```\n",
    "Fills missing:\n",
    "- `open` values with `0`\n",
    "- `close` values with `0.5`\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ Use another column to fill missing values:\n",
    "```python\n",
    "df[\"open\"].fillna(df[\"close\"], inplace=True)\n",
    "```\n",
    "Fills NaNs in `open` with the corresponding `close` values.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. üìä Grouping Data (`.groupby()`)\n",
    "\n",
    "#### Similar to SQL‚Äôs `GROUP BY`:\n",
    "\n",
    "SQL Example:\n",
    "```sql\n",
    "SELECT ticker, AVG(open), AVG(close)\n",
    "FROM stock_data\n",
    "GROUP BY ticker;\n",
    "```\n",
    "\n",
    "#### pandas equivalent:\n",
    "```python\n",
    "grouped_df = df.groupby(\"ticker\").mean()\n",
    "```\n",
    "\n",
    "Other aggregations:\n",
    "```python\n",
    "df.groupby(\"ticker\").min()\n",
    "df.groupby(\"ticker\").max()\n",
    "df.groupby(\"ticker\").sum()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. üõ†Ô∏è Custom Transformations (`.apply()`)\n",
    "\n",
    "When built-in methods aren‚Äôt enough, define a function and use `.apply()`.\n",
    "\n",
    "#### Example: Classify stock price movement\n",
    "```python\n",
    "def classify_change(row):\n",
    "    if row[\"close\"] > row[\"open\"]:\n",
    "        return \"Increase\"\n",
    "    else:\n",
    "        return \"Decrease\"\n",
    "\n",
    "df[\"change\"] = df.apply(classify_change, axis=1)\n",
    "```\n",
    "\n",
    "‚úÖ Setting `axis=1` ensures that the function is applied **row-wise**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary Table\n",
    "\n",
    "| Task                        | Method              | Description |\n",
    "|-----------------------------|---------------------|-------------|\n",
    "| Fill all NaNs               | `df.fillna(0)`      | Replaces all missing values |\n",
    "| Fill column-specific NaNs   | `df.fillna({...})`  | Dictionary of column-value pairs |\n",
    "| Fill using another column   | `df[col1].fillna(df[col2])` | Row-wise filling |\n",
    "| Group by and aggregate      | `df.groupby(\"col\")` | Similar to SQL GROUP BY |\n",
    "| Custom logic per row        | `df.apply(func, axis=1)` | Apply complex functions |\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Practice Tip:\n",
    "Try combining all three techniques (filling, grouping, and applying custom logic) on a small dataset to simulate a **mini ETL pipeline**.\n",
    "\n",
    "Want me to give you a practice JSON or a mini challenge for this topic?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1b81d3",
   "metadata": {},
   "source": [
    "### **Filling missing values with pandas**\n",
    "When building data pipelines, it's inevitable that you'll stumble upon missing data. In some cases, you may want to remove these records from the dataset. But in others, you'll need to impute values for the missing information. In this exercise, you'll practice using pandas to impute missing test scores.\n",
    "\n",
    "Data from the file \"testing_scores.json\" has been read into a DataFrame, and is stored in the variable raw_testing_scores. In addition to this, pandas has been loaded as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9e1ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with the average from that column\n",
    "raw_testing_scores[\"math_score\"] = raw_testing_scores[\"math_score\"].fillna(raw_testing_scores[\"math_score\"].mean())\n",
    "\n",
    "# Print the head of the raw_testing_scores DataFrame\n",
    "print(raw_testing_scores.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcb0736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(raw_data):\n",
    "\traw_data.fillna(\n",
    "    \tvalue={\n",
    "\t\t\t# Fill NaN values with column mean\n",
    "\t\t\t\"math_score\": raw_data[\"math_score\"].mean(),\n",
    "\t\t\t\"reading_score\": raw_data[\"reading_score\"].mean(),\n",
    "\t\t\t\"writing_score\": raw_data[\"writing_score\"].mean(),\n",
    "\t\t}, inplace=True\n",
    "\t)\n",
    "\treturn raw_data\n",
    "\n",
    "clean_testing_scores = transform(raw_testing_scores)\n",
    "\n",
    "# Print the head of the clean_testing_scores DataFrame\n",
    "print(clean_testing_scores.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae06ff3",
   "metadata": {},
   "source": [
    "### **Grouping data with pandas**\n",
    "The output of a data pipeline is typically a \"modeled\" dataset. This dataset provides data consumers easy access to information, without having to perform much manipulation. Grouping data with pandas helps to build modeled datasets,\n",
    "\n",
    "pandas has been imported as pd, and the raw_testing_scores DataFrame contains data in the following form:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a147c87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(raw_data):\n",
    "\t# Use .loc[] to only return the needed columns\n",
    "\traw_data = raw_data.loc[:, ['city','math_score','reading_score', 'writing_score']]\n",
    "\t\n",
    "    # Group the data by city, return the grouped DataFrame\n",
    "\tgrouped_data = raw_data.groupby(by=[\"city\"], axis=0).mean()\n",
    "\treturn grouped_data\n",
    "\n",
    "# Transform the data, print the head of the DataFrame\n",
    "grouped_testing_scores = transform(raw_testing_scores)\n",
    "print(grouped_testing_scores.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f34e1e4",
   "metadata": {},
   "source": [
    "### **Applying advanced transformations to DataFrames**\n",
    "pandas has a plethora of built-in transformation tools, but sometimes, more advanced logic needs to be used in a transformation. The apply function lets you apply a user-defined function to a row or column of a DataFrame, opening the door for advanced transformation and feature generation.\n",
    "\n",
    "The find_street_name() function parses the street name from the \"street_address\", dropping the street number from the string. This function has been loaded into memory, and is ready to be applied to the raw_testing_scores DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65eb2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(raw_data):\n",
    "\t# Use the apply function to extract the street_name from the street_address\n",
    "    raw_data[\"street_name\"] = raw_data.apply(\n",
    "   \t\t# Pass the correct function to the apply method\n",
    "        find_street_name,\n",
    "        axis=1\n",
    "    )\n",
    "    return raw_data\n",
    "\n",
    "# Transform the raw_testing_scores DataFrame\n",
    "cleaned_testing_scores = transform(raw_testing_scores)\n",
    "\n",
    "# Print the head of the cleaned_testing_scores DataFrame\n",
    "print(cleaned_testing_scores.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41610c8c",
   "metadata": {},
   "source": [
    "Here are your final and comprehensive notes for this chapter on **Loading Data to a SQL Database with pandas**, the final step in the ETL process:\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Loading Data to SQL with pandas ‚Äî Notes\n",
    "\n",
    "After extracting and transforming data, the final ETL step is **loading the cleaned data** into a SQL database for downstream use in analytics and reporting.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. üì• `.to_sql()` ‚Äî Load DataFrame to SQL\n",
    "\n",
    "```python\n",
    "df.to_sql(\n",
    "    name=\"table_name\",       # SQL table name\n",
    "    con=engine,              # SQLAlchemy engine (connection object)\n",
    "    if_exists=\"append\",      # Options: 'fail', 'replace', 'append'\n",
    "    index=True,              # Include the DataFrame index\n",
    "    index_label=\"id_column\"  # Label for index column in SQL\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. üîó Create SQLAlchemy Engine\n",
    "\n",
    "To connect to a SQL database (e.g., **PostgreSQL**), use SQLAlchemy:\n",
    "\n",
    "```python\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "db_uri = \"postgresql://username:password@host:port/database\"\n",
    "engine = create_engine(db_uri)\n",
    "```\n",
    "\n",
    "üìå **Example**:\n",
    "```python\n",
    "engine = create_engine(\"postgresql://user:pass@localhost:5432/market\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. üßæ Example: Persisting Data\n",
    "\n",
    "```python\n",
    "clean_stock_data.to_sql(\n",
    "    name=\"filtered_stock_data\",\n",
    "    con=engine,\n",
    "    if_exists=\"append\",\n",
    "    index=True,\n",
    "    index_label=\"timestamps\"\n",
    ")\n",
    "```\n",
    "\n",
    "- `name`: **filtered_stock_data** ‚Äî Table name\n",
    "- `con`: **engine** ‚Äî Connection to the Postgres database\n",
    "- `if_exists`: **append** ‚Äî Adds to existing table\n",
    "- `index`: **True** ‚Äî Writes index to SQL\n",
    "- `index_label`: **timestamps** ‚Äî SQL column for index\n",
    "\n",
    "---\n",
    "\n",
    "### 4. ‚úÖ Validating Data Load\n",
    "\n",
    "Once data is loaded, validation is critical:\n",
    "\n",
    "```python\n",
    "# Read from SQL to validate\n",
    "sql_df = pd.read_sql(\"SELECT * FROM filtered_stock_data\", con=engine)\n",
    "\n",
    "# Compare with original DataFrame\n",
    "assert len(sql_df) == len(clean_stock_data)\n",
    "assert sql_df.equals(clean_stock_data)\n",
    "```\n",
    "\n",
    "#### Best Practices:\n",
    "- ‚úÖ Row counts should match\n",
    "- ‚úÖ Each row‚Äôs values should be identical\n",
    "- ‚úÖ Perform manual data quality checks\n",
    "- ‚úÖ Add this validation to monitoring pipelines to **instill trust**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary Table\n",
    "\n",
    "| Task                            | Tool/Method           | Example |\n",
    "|---------------------------------|------------------------|---------|\n",
    "| Connect to SQL DB               | `create_engine()`      | `engine = create_engine(...)` |\n",
    "| Load DataFrame to SQL           | `.to_sql()`            | `df.to_sql(name=\"table\", con=engine)` |\n",
    "| Validate data in SQL            | `pd.read_sql()`        | `pd.read_sql(\"SELECT * ...\", con=engine)` |\n",
    "| Compare with original DataFrame | `.equals()`, `len()`   | `assert df1.equals(df2)` |\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Real-World Application\n",
    "Once persisted in SQL, your data is now:\n",
    "- Ready for **visualization tools** (e.g., Power BI, Tableau)\n",
    "- Queryable using standard **SQL**\n",
    "- Accessible for other **data consumers** and **systems**\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want a hands-on example with a SQLite/Postgres setup or help integrating it into a full pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf79d4cf",
   "metadata": {},
   "source": [
    "### **Loading data to a Postgres database**\n",
    "After data has been extracted from a source system and transformed to align with analytics or reporting use cases, it's time to load the data to a final storage medium. Storing cleaned data in a SQL database makes it simple for data consumers to access and run queries against. In this example, you'll practice loading cleaned data to a Postgres database.\n",
    "\n",
    "sqlalchemy has been imported, and pandas is available as pd. The first few rows of the cleaned_testing_scores DataFrame are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5740a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the connection string, create the connection object to the schools database\n",
    "db_engine = sqlalchemy.create_engine(\"postgresql+psycopg2://repl:password@localhost:5432/schools\")\n",
    "\n",
    "# Write the DataFrame to the scores table\n",
    "cleaned_testing_scores.to_sql(\n",
    "\tname=\"scores\",\n",
    "\tcon=db_engine,\n",
    "\tindex=False,\n",
    "\tif_exists=\"replace\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3baf08",
   "metadata": {},
   "source": [
    "### **Validating data loaded to a Postgres Database**\n",
    "In this exercise, you'll finally get to build a data pipeline from end-to-end. This pipeline will extract school testing scores from a JSON file and transform the data to drop rows with missing scores. In addition to this, each will be ranked by the city they are located in, based on their total scores. Finally, the transformed dataset will be stored in a Postgres database.\n",
    "\n",
    "To give you a head start, the extract() and transform() functions have been built and used as shown below. In addition to this, pandas has been imported as pd. Best of luck!\n",
    "``` python\n",
    "# Extract and clean the testing scores.\n",
    "raw_testing_scores = extract(\"testing_scores.json\")\n",
    "cleaned_testing_scores = transform(raw_testing_scores)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e891aefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the load() function to write the clean_data DataFrame to the scores_by_city table in the schools database.\n",
    "# If data exists in the scores_by_city table, makes sure to replace it with the updated data.\n",
    "def load(clean_data, con_engine):\n",
    "\t# Store the data in the schools database\n",
    "    clean_data.to_sql(\n",
    "    \tname=\"scores_by_city\",\n",
    "\t\tcon=con_engine,\n",
    "\t\tif_exists=\"replace\",  # Make sure to replace existing data\n",
    "\t\tindex=True,\n",
    "\t\tindex_label=\"school_id\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the data from the cleaned_testing_scores, using the db_engine that has already been defined.\n",
    "# Use pandas to read data from the scores_by_city table, and print the first few rows of the DataFrame to validate that data was persisted.\n",
    "\n",
    "def load(clean_data, con_engine):\n",
    "    clean_data.to_sql(name=\"scores_by_city\", con=con_engine, if_exists=\"replace\", index=True, index_label=\"school_id\")\n",
    "    \n",
    "# Call the load function, passing in the cleaned DataFrame\n",
    "load(cleaned_testing_scores, db_engine)\n",
    "\n",
    "# Call query the data in the scores_by_city table, check the head of the DataFrame\n",
    "to_validate = pd.read_sql(\"SELECT * FROM scores_by_city\", con=db_engine)\n",
    "print(to_validate.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bc563d",
   "metadata": {},
   "source": [
    "### **Creating fixtures with pytest**\n",
    "When building unit tests, you'll sometimes have to do a bit of setup before testing can begin. Doing this setup within a unit test can make the tests more difficult to read, and may have to be repeated several times. Luckily, pytest offers a way to solve these problems, with fixtures.\n",
    "\n",
    "For this exercise, pandas has been imported as pd, and the extract() function shown below is available for use!\n",
    "```python\n",
    "def extract(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c9d30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pytest\n",
    "import pytest\n",
    "\n",
    "# Create a pytest fixture\n",
    "@pytest.fixture()\n",
    "def raw_tax_data():\n",
    "\traw_data = extract(\"raw_tax_data.csv\")\n",
    "   \n",
    "    # Return the raw DataFrame\n",
    "\treturn raw_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa3584c",
   "metadata": {},
   "source": [
    "### **Unit testing a data pipeline with fixtures**\n",
    "You've learned in the last video that unit testing can help to instill more trust in your data pipeline, and can even help to catch bugs throughout development. In this exercise, you'll practice writing both fixtures and unit tests, using the pytest library and assert.\n",
    "\n",
    "The transform function that you'll be building unit tests around is shown below for reference. pandas has been imported as pd, and the pytest() library is loaded and ready for use.\n",
    "\n",
    "```python\n",
    "def transform(raw_data):\n",
    "    raw_data[\"tax_rate\"] = raw_data[\"total_taxes_paid\"] / raw_data[\"total_taxable_income\"]\n",
    "    raw_data.set_index(\"industry_name\", inplace=True)\n",
    "    return raw_data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d4c6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pytest fixture\n",
    "@pytest.fixture()\n",
    "def clean_tax_data():\n",
    "    raw_data = pd.read_csv(\"raw_tax_data.csv\")\n",
    "    \n",
    "    # Transform the raw_data, store in clean_data DataFrame, and return the variable\n",
    "    clean_data = transform(raw_data)\n",
    "    return clean_data\n",
    "\n",
    "# Pass the fixture to the function\n",
    "def test_tax_rate(clean_tax_data):\n",
    "    # Assert values are within the expected range\n",
    "    assert clean_tax_data[\"tax_rate\"].max() <= 1 and clean_tax_data[\"tax_rate\"].min() >= 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb8fa83",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c96f52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56f8b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
