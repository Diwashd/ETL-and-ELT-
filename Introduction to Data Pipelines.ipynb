{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6873f040",
   "metadata": {},
   "source": [
    "### Simplified Notes on ETL and ELT Pipelines\n",
    "\n",
    "#### 1. **Introduction**\n",
    "- The course is about building ETL (Extract, Transform, Load) and ELT (Extract, Load, Transform) data pipelines.\n",
    "- Jake, the instructor, uses tools like Python, SQL, and Airflow to build these pipelines.\n",
    "\n",
    "#### 2. **BI, ML, and AI**\n",
    "- **Business Intelligence (BI)**, **Machine Learning (ML)**, and **AI** require data to be well-prepared.\n",
    "- Data pipelines ensure data is in the right place, shape, and time for analysis or machine learning models.\n",
    "\n",
    "#### 3. **Data Pipelines**\n",
    "- Data pipelines move data from source systems (e.g., CSV, APIs, databases) to a destination.\n",
    "- During this process, data is **extracted**, **transformed** (cleaned and modified), and then **loaded** into systems like data warehouses or BI tools.\n",
    "\n",
    "#### 4. **ETL vs. ELT Pipelines**\n",
    "- **ETL (Extract, Transform, Load)**:\n",
    "  - Data is extracted, transformed (cleaned, structured), and then loaded into a destination.\n",
    "  - ETL is commonly used with tools like Python and pandas for data manipulation.\n",
    "  \n",
    "- **ELT (Extract, Load, Transform)**:\n",
    "  - Data is extracted, loaded into the destination (e.g., data warehouse), and then transformed after loading.\n",
    "  - ELT is often used with large-scale data in data warehouses.\n",
    "\n",
    "#### 5. **ETL Pipeline Example**\n",
    "- The ETL pipeline uses three functions:\n",
    "  1. **Extract**: Pull data from a source.\n",
    "  2. **Transform**: Clean and modify data using Python.\n",
    "  3. **Load**: Write data to a SQL database.\n",
    "\n",
    "#### 6. **ELT Pipeline Example**\n",
    "- The ELT pipeline also uses three functions:\n",
    "  1. **Extract**: Pull data from a source.\n",
    "  2. **Load**: Load data into a destination (like a data warehouse).\n",
    "  3. **Transform**: Perform data transformation after loading, usually with SQL queries.\n",
    "\n",
    "#### 7. **Additional Learning**\n",
    "- The course will cover both **tabular** (structured) and **non-tabular** (unstructured) data.\n",
    "- **Pandas** is used for data transformation and writing data to SQL databases.\n",
    "- Topics like **unit testing**, **monitoring**, and **deploying pipelines to production** will also be covered.\n",
    "\n",
    "#### 8. **Hands-On Practice**\n",
    "- The course encourages building real-world ETL and ELT pipelines through practical exercises.\n",
    "\n",
    "---\n",
    "\n",
    "These notes give a clear and simple overview of the ETL and ELT pipeline concepts and practices, making it easier to understand and remember."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f36d99",
   "metadata": {},
   "source": [
    "### **Running an ETL Pipeline**\n",
    "Ready to run your first ETL pipeline? Let's get to it!\n",
    "\n",
    "Here, the functions extract(), transform(), and load() have been defined for you. To run this data ETL pipeline, you're going to execute each of these functions. If you're curious, take a peek at what the extract() function looks like.\n",
    "```\n",
    "def extract(file_name):\n",
    "    print(f\"Extracting data from {file_name}\")\n",
    "    return pd.read_csv(file_name)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1bbbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from the raw_data.csv file\n",
    "extracted_data = extract(file_name=\"raw_data.csv\")\n",
    "\n",
    "# Transform the extracted_data\n",
    "transformed_data = transform(data_frame=extracted_data)\n",
    "\n",
    "# Load the transformed_data to cleaned_data.csv\n",
    "load(data_frame=transformed_data, target_table=\"cleaned_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e25d185",
   "metadata": {},
   "source": [
    "### **ELT in Action**\n",
    "Feeling pretty good about running ETL processes? Well, it's time to give ELT pipelines a try. Like before, the extract(), load(), and transform() functions have been defined for you; all you'll have to worry about is running these functions. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23e400e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from the raw_data.csv file\n",
    "raw_data = extract(file_name=\"raw_data.csv\")\n",
    "\n",
    "# Load the extracted_data to the raw_data table\n",
    "load(data_frame=raw_data, table_name=\"raw_data\")\n",
    "\n",
    "# Transform data in the raw_data table\n",
    "transform(\n",
    "  source_table=\"raw_data\", \n",
    "  target_table=\"cleaned_data\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9c73dc",
   "metadata": {},
   "source": [
    "### Simplified Notes on Building ETL and ELT Pipelines – Part 2\n",
    "\n",
    "#### 1. **Overview**\n",
    "- We’re now diving deeper into how ETL and ELT pipelines are built and run in practice.\n",
    "- The focus is on using Python and pandas to extract, transform, and load data.\n",
    "\n",
    "#### 2. **Extracting Data from CSV**\n",
    "- Use the `pandas` library to read data from a CSV file.\n",
    "- Function used: `pd.read_csv(\"file_path\")`\n",
    "- Optional parameters include:\n",
    "  - `delimiter`: defines how data is separated\n",
    "  - `header`: specifies the row containing column names\n",
    "  - `engine`: helps with file compatibility\n",
    "- Use `.head()` to preview the first few rows of the DataFrame (default is 5).\n",
    "\n",
    "#### 3. **Filtering a DataFrame**\n",
    "- After extracting, the data can be filtered using `.loc[]`.\n",
    "- Example usage:\n",
    "  - `df.loc[df[\"name\"] == \"Apparel\", :]` → filters rows where `name` is \"Apparel\"\n",
    "  - `df.loc[:, [\"name\", \"num_firms\"]]` → selects only the specified columns\n",
    "- `loc` is powerful for filtering rows and columns at the same time.\n",
    "\n",
    "#### 4. **Writing a DataFrame to CSV**\n",
    "- Use `.to_csv(\"file_path\")` to write a DataFrame back to a CSV file.\n",
    "- Just like `read_csv`, `to_csv` also has optional settings to control the output format.\n",
    "- Other formats for exporting data:\n",
    "  - `.to_json()`\n",
    "  - `.to_excel()`\n",
    "  - `.to_sql()` (for databases)\n",
    "\n",
    "#### 5. **Using SQL for Transformation**\n",
    "- You can run SQL queries directly from Python using connectors like:\n",
    "  - SQLAlchemy\n",
    "  - Snowflake Connector\n",
    "- SQL queries are often written as multi-line strings.\n",
    "- Use methods like `.execute()` to run queries and create tables in the database.\n",
    "\n",
    "#### 6. **Combining ETL Steps**\n",
    "- Each step (extract, transform, load) can be defined in separate Python functions.\n",
    "- Example ETL flow:\n",
    "  1. `extract()` reads data from a CSV.\n",
    "  2. `transform()` filters rows with `name == \"Apparel\"`.\n",
    "  3. `load()` saves the result to `cleaned_data.csv`.\n",
    "- This structure will be used throughout the course for building pipelines.\n",
    "\n",
    "#### 7. **Practice Time**\n",
    "- You’re encouraged to try out these tools and build a small ETL/ELT pipeline on your own now.\n",
    "\n",
    "---\n",
    "\n",
    "These notes break down the process of building pipelines using pandas and SQL, making it easier to remember the key tools and steps involved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40c2357",
   "metadata": {},
   "source": [
    "### **Running an ETL Pipeline**\n",
    "Ready to run your first ETL pipeline? Let's get to it!\n",
    "\n",
    "Here, the functions extract(), transform(), and load() have been defined for you. To run this data ETL pipeline, you're going to execute each of these functions. If you're curious, take a peek at what the extract() function looks like.\n",
    "\n",
    "```\n",
    "def extract(file_name):\n",
    "    print(f\"Extracting data from {file_name}\")\n",
    "    return pd.read_csv(file_name)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e660325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from the raw_data.csv file\n",
    "extracted_data = extract(file_name=\"raw_data.csv\")\n",
    "\n",
    "# Transform the extracted_data\n",
    "transformed_data = transform(data_frame=extracted_data)\n",
    "\n",
    "# Load the transformed_data to cleaned_data.csv\n",
    "load(data_frame=transformed_data, target_table=\"cleaned_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496aafda",
   "metadata": {},
   "source": [
    "### **ELT in Action**\n",
    "Feeling pretty good about running ETL processes? Well, it's time to give ELT pipelines a try. Like before, the extract(), load(), and transform() functions have been defined for you; all you'll have to worry about is running these functions. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0a1161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from the raw_data.csv file\n",
    "raw_data = extract(file_name=\"raw_data.csv\")\n",
    "\n",
    "# Load the extracted_data to the raw_data table\n",
    "load(data_frame=raw_data, table_name=\"raw_data\")\n",
    "\n",
    "# Transform data in the raw_data table\n",
    "transform(\n",
    "  source_table=\"raw_data\", \n",
    "  target_table=\"cleaned_data\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e143fa",
   "metadata": {},
   "source": [
    "### **Building an ETL Pipeline**\n",
    "Ready to ratchet up the fun? In this exercise, you'll be responsible for building the rest of the load() function before running each step in the ETL process. The extract() and transform() functions have been defined for you. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1a5caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(data_frame, file_name):\n",
    "  # Write cleaned_data to a CSV using file_name\n",
    "  data_frame.to_csv(file_name)\n",
    "  print(f\"Successfully loaded data to {file_name}\")\n",
    "\n",
    "extracted_data = extract(file_name=\"raw_data.csv\")\n",
    "\n",
    "# Transform extracted_data using transform() function\n",
    "transformed_data = transform(data_frame=extracted_data)\n",
    "\n",
    "# Load transformed_data to the file transformed_data.csv\n",
    "load(data_frame=transformed_data, file_name=\"transformed_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d11859",
   "metadata": {},
   "source": [
    "### **The \"T\" in ELT**\n",
    "Let's not forget about ELT! Here, the extract() and load() functions have been defined for you. Now, all that's left is to finish defining the transform() function and run the pipeline. Go get 'em!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d79982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete building the transform() function\n",
    "def transform(source_table, target_table):\n",
    "  data_warehouse.execute(f\"\"\"\n",
    "  CREATE TABLE {target_table} AS\n",
    "      SELECT\n",
    "          CONCAT(\"Product ID: \", product_id),\n",
    "          quantity * price\n",
    "      FROM {source_table};\n",
    "  \"\"\")\n",
    "\n",
    "extracted_data = extract(file_name=\"raw_sales_data.csv\")\n",
    "load(data_frame=extracted_data, table_name=\"raw_sales_data\")\n",
    "\n",
    "# Populate total_sales by transforming raw_sales_data\n",
    "transform(source_table=\"raw_sales_data\", target_table=\"total_sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abef182e",
   "metadata": {},
   "source": [
    "### **Extracting, Transforming, and Loading Student Scores Data**\n",
    "Alright, it's time to build your own ETL pipeline from scratch. In this exercise, you'll build three functions; extract(), transform(), and load(). Then, you'll use these functions to run your pipeline.\n",
    "\n",
    "The pandas library has been imported as pd. Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da7d1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(file_name):\n",
    "  # Read a CSV with a path stored using file_name into memory\n",
    "  return pd.read_csv(file_name)\n",
    "\n",
    "\n",
    "def transform(data_frame):\n",
    "  # Filter the data_frame to only incude a subset of columns\n",
    "  return data_frame.loc[:, [\"industry_name\", \"number_of_firms\"]]\n",
    "\n",
    "\n",
    "def load(data_frame, file_name):\n",
    "  # Write the data_frame to a CSV\n",
    "  data_frame.to_csv(file_name)\n",
    "\n",
    "\n",
    "extracted_data = extract(file_name=\"raw_industry_data.csv\")\n",
    "transformed_data = transform(data_frame=extracted_data)\n",
    "\n",
    "# Pass the transformed_data DataFrame to the load() function\n",
    "load(data_frame=transformed_data, file_name=\"number_of_firms.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546b23dd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ff03759",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d02dda3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "353fbfe5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b46a994",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
